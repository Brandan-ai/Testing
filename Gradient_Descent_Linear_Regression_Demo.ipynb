{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Demo \u2014 Linear Regression\n",
    "\n",
    "This notebook shows **gradient descent** finding a best-fit line for a small dataset.\n",
    "\n",
    "### Simple idea (student-friendly):\n",
    "- Imagine you're on a hill in the dark. You take **small steps downhill** until you reach the bottom.\n",
    "- Here, the \"hill\" is the error (Mean Squared Error). The \"bottom\" is the smallest error.\n",
    "- We update the slope `m` and intercept `b` step by step to go **downhill**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data\n",
    "We'll reuse a simple dataset (hours studied vs exam score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "y = np.array([50, 55, 65, 70, 75], dtype=float)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Hours studied')\n",
    "plt.ylabel('Exam score')\n",
    "plt.title('Data (hours vs score)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Gradient Descent Functions\n",
    "We'll implement the update rules for `m` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, m, b):\n",
    "    y_pred = m * X + b\n",
    "    error = y - y_pred\n",
    "    dm = -(2/len(X)) * np.sum(X * error)\n",
    "    db = -(2/len(X)) * np.sum(error)\n",
    "    return dm, db\n",
    "\n",
    "def mse(X, y, m, b):\n",
    "    return np.mean((y - (m*X + b))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Run Gradient Descent\n",
    "We start with guesses for `m` and `b`, then take many small steps. Try changing `alpha` (learning rate) and `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01   # learning rate (step size)\n",
    "epochs = 1000  # number of steps\n",
    "m, b = 0.0, 0.0\n",
    "\n",
    "history = {'m': [], 'b': [], 'loss': []}\n",
    "\n",
    "for _ in range(epochs):\n",
    "    dm, db = compute_gradients(X, y, m, b)\n",
    "    m -= alpha * dm\n",
    "    b -= alpha * db\n",
    "    history['m'].append(m)\n",
    "    history['b'].append(b)\n",
    "    history['loss'].append(mse(X, y, m, b))\n",
    "\n",
    "print('Final slope (m):', round(m, 4))\n",
    "print('Final intercept (b):', round(b, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Loss Curve (Are we going downhill?)\n",
    "If gradient descent is working, the loss (MSE) should go down over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE (loss)')\n",
    "plt.title('Loss decreasing over time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) See the Line Improve (Early \u2192 Middle \u2192 Final)\n",
    "We plot the fitted line at three moments in training. One plot per cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_y(m, b, X):\n",
    "    return m*X + b\n",
    "\n",
    "x_line = np.linspace(X.min(), X.max(), 100)\n",
    "\n",
    "# Early training (epoch ~ 10)\n",
    "m10, b10 = history['m'][9], history['b'][9]\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_line, line_y(m10, b10, x_line))\n",
    "plt.xlabel('Hours studied')\n",
    "plt.ylabel('Exam score')\n",
    "plt.title('Early fit (~epoch 10)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Middle training (epoch ~ 100)\n",
    "m100, b100 = history['m'][99], history['b'][99]\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_line, line_y(m100, b100, x_line))\n",
    "plt.xlabel('Hours studied')\n",
    "plt.ylabel('Exam score')\n",
    "plt.title('Middle fit (~epoch 100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training (epoch ~ 1000)\n",
    "mF, bF = history['m'][-1], history['b'][-1]\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_line, line_y(mF, bF, x_line))\n",
    "plt.xlabel('Hours studied')\n",
    "plt.ylabel('Exam score')\n",
    "plt.title('Final fit (~epoch 1000)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Compare to scikit-learn's LinearRegression (closed-form)\n",
    "This shows the slope & intercept found by the **Normal Equation** method used by scikit-learn (no gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X_2d = X.reshape(-1, 1)\n",
    "sk = LinearRegression().fit(X_2d, y)\n",
    "print('sklearn slope:', sk.coef_[0])\n",
    "print('sklearn intercept:', sk.intercept_)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X, y)\n",
    "plt.plot(x_line, sk.predict(x_line.reshape(-1,1)))\n",
    "plt.xlabel('Hours studied')\n",
    "plt.ylabel('Exam score')\n",
    "plt.title('scikit-learn best-fit line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Try it yourself\n",
    "- Change `alpha` and see if the loss still goes down. Too big can make it diverge.\n",
    "- Change `epochs` to see how many steps you need.\n",
    "- Add noise to `y` and see how the fit changes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}