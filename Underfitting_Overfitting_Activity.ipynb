{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Underfitting vs Overfitting\n",
    "\n",
    "In this activity, you'll explore what happens when we fit models of different complexity to data.\n",
    "\n",
    "## Steps:\n",
    "1. Generate synthetic data (quadratic pattern + noise).\n",
    "2. Fit three models:\n",
    "   - Linear (underfit)\n",
    "   - Quadratic (good fit)\n",
    "   - High-degree polynomial (overfit)\n",
    "3. Compare the plots.\n",
    "4. Check training vs test errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Generate data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 40).reshape(-1, 1)\n",
    "y = 0.5 * X[:,0]**2 + np.random.randn(40) * 1.5\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "plt.scatter(X_train, y_train, label='Train data')\n",
    "plt.scatter(X_test, y_test, label='Test data')\n",
    "plt.legend()\n",
    "plt.title('Generated Data (Quadratic with noise)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit models of different complexity\n",
    "- Linear (degree=1)\n",
    "- Quadratic (degree=2)\n",
    "- Overfit (degree=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot(degree):\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "    X_poly_test = poly.transform(X_test)\n",
    "    model = LinearRegression().fit(X_poly_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_poly_train)\n",
    "    y_test_pred = model.predict(X_poly_test)\n",
    "    \n",
    "    # Errors\n",
    "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "    \n",
    "    # Plot\n",
    "    X_line = np.linspace(-3, 3, 200).reshape(-1,1)\n",
    "    y_line = model.predict(poly.transform(X_line))\n",
    "    plt.scatter(X_train, y_train, label='Train data')\n",
    "    plt.scatter(X_test, y_test, label='Test data')\n",
    "    plt.plot(X_line, y_line, color='red', label=f'Degree {degree}')\n",
    "    plt.title(f'Polynomial degree {degree}\\nTrain MSE={train_mse:.2f}, Test MSE={test_mse:.2f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "for d in [1, 2, 15]:\n",
    "    fit_and_plot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Discussion Questions\n",
    "1. Which model underfits the data? Why?\n",
    "2. Which model overfits the data? How can you tell?\n",
    "3. Why is the quadratic model the best balance?\n",
    "4. What do you notice about **train vs test error** for each case?\n",
    "5. How could we avoid overfitting in real-world problems?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}